{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31b57b9-c8cb-4244-b093-519f65081846",
   "metadata": {},
   "source": [
    "# 2. Dataset Setup & Exploration\n",
    "**Objective:** Download and inspect the Intel Image Classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b554f969-56c0-4441-8bc1-e24826423ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/jovyan/.local/lib/python3.12/site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jovyan/.local/lib/python3.12/site-packages (from tensorflow) (1.70.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.0.2)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Downloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 h5py-3.13.0 keras-3.9.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.1 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/jovyan/.local/lib/python3.12/site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jovyan/.local/lib/python3.12/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Collecting kaggle\n",
      "  Using cached kaggle-1.6.17-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.12/site-packages (from kaggle) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from kaggle) (4.67.1)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.12/site-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.12/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->kaggle) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->kaggle) (3.10)\n",
      "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.6.17 python-slugify-8.0.4 text-unidecode-1.3\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (11.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 16:48:25.981523: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-07 16:48:25.985782: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-07 16:48:25.997757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741366106.018758     425 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741366106.024879     425 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-07 16:48:26.045627: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Set up necessary installs\n",
    "%pip install tensorflow\n",
    "!pip install tensorflow\n",
    "!pip install kaggle\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install pillow\n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import Model,layers\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dbad066-63af-42c1-8b11-ba1a2bc0b00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/mks/Predictive Analytics/Individual Assignment/Image Classification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "print(\"Current working directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb99ec32-d2ac-429f-b008-72f59b19b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json has been moved to /Users/mks/Predictive Analytics/Individual Assignment/Image Classification/.kaggle/kaggle.json.\n"
     ]
    }
   ],
   "source": [
    "# Create a Kaggle API key\n",
    "# The file should initially be in the current directory where the command is executed.\n",
    "# Moving key to proper location\n",
    "import os\n",
    "\n",
    "# Define the current and target paths\n",
    "current_directory = os.getcwd()\n",
    "kaggle_json_current = 'kaggle.json'  # Assuming the file is in the current directory\n",
    "kaggle_json_target = os.path.join(current_directory, '.kaggle', 'kaggle.json')\n",
    "\n",
    "try:\n",
    "    # Check if kaggle.json is in the current directory\n",
    "    if os.path.exists(kaggle_json_current):\n",
    "        # Create the .kaggle directory in the current directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(kaggle_json_target), exist_ok=True)\n",
    "        \n",
    "        # Move the file to the new .kaggle directory\n",
    "        os.rename(kaggle_json_current, kaggle_json_target)\n",
    "        \n",
    "        print(f\"kaggle.json has been moved to {kaggle_json_target}.\")\n",
    "    \n",
    "    elif os.path.exists(kaggle_json_target):\n",
    "        print(\"kaggle.json already exists in the target location within the current directory.\")\n",
    "\n",
    "    else:\n",
    "        print(\"kaggle.json not found in the current directory or the target location. Please ensure the file is available.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c813dee-4ab9-413c-81fd-8bd5de1de239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/puneet6060/intel-image-classification\n",
      "License(s): copyright-authors\n",
      "Downloading intel-image-classification.zip to /Users/mks/Predictive Analytics/Individual Assignment/Image Classification\n",
      "100%|███████████████████████████████████████▊| 345M/346M [00:08<00:00, 51.0MB/s]\n",
      "100%|████████████████████████████████████████| 346M/346M [00:08<00:00, 43.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Install Kaggle API\n",
    "!pip install -q kaggle\n",
    "!kaggle datasets download -d puneet6060/intel-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b6df3ec-951a-4bf7-a903-4ac77184395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'intel_dataset' not found. Unzipping dataset...\n",
      "Unzip complete.\n"
     ]
    }
   ],
   "source": [
    "#unzip the files if they are not already unzipped\n",
    "\n",
    "import os\n",
    "\n",
    "target_dir = \"intel_dataset\"\n",
    "zip_file = \"intel-image-classification.zip\"\n",
    "\n",
    "if not os.path.isdir(target_dir):\n",
    "    print(f\"Directory '{target_dir}' not found. Unzipping dataset...\")\n",
    "    # Using the shell command with error handling\n",
    "    try:\n",
    "        get_ipython().system(f\"unzip -q {zip_file} -d {target_dir}\")\n",
    "        print(\"Unzip complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while unzipping: {e}\")\n",
    "else:\n",
    "    print(f\"Directory '{target_dir}' already exists. Skipping unzip.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27640211-d778-4d3f-858d-68ef62fbe144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In our directory, we current have these downloaded:\n",
      "\n",
      "Train set counts:\n",
      "{'forest': 2271, 'buildings': 2191, 'glacier': 2404, 'street': 2382, 'mountain': 2512, 'sea': 2274}\n",
      "\n",
      "Test set counts:\n",
      "{'forest': 474, 'buildings': 437, 'glacier': 553, 'street': 501, 'mountain': 525, 'sea': 510}\n",
      "\n",
      "Prediction set counts:\n",
      "{'seg_pred': 7301}\n",
      "\n",
      "Cross check these numbers against the Kaggle website datacard to ensure we have all images downloaded.\n"
     ]
    }
   ],
   "source": [
    "# For a check on whether each of the folder is downloaded\n",
    "#Count of images in each folder\n",
    "import os\n",
    "\n",
    "def count_images_in_directory(directory):\n",
    "    counts = {}\n",
    "    # Loop over each subfolder (class) in the given directory\n",
    "    for folder in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Count common image files\n",
    "            files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            counts[folder] = len(files)\n",
    "    return counts\n",
    "\n",
    "# Define paths to the seg_train, seg_test, and seg_pred directories\n",
    "train_dir = os.path.join('intel_dataset', 'seg_train', 'seg_train')\n",
    "test_dir = os.path.join('intel_dataset', 'seg_test', 'seg_test')\n",
    "pred_dir = os.path.join('intel_dataset', 'seg_pred')\n",
    "\n",
    "print(\"\\nIn our directory, we current have these downloaded:\")\n",
    "print(\"\\nTrain set counts:\")\n",
    "print(count_images_in_directory(train_dir))\n",
    "\n",
    "print(\"\\nTest set counts:\")\n",
    "print(count_images_in_directory(test_dir))\n",
    "\n",
    "print(\"\\nPrediction set counts:\")\n",
    "print(count_images_in_directory(pred_dir))\n",
    "\n",
    "print(\"\\nCross check these numbers against the Kaggle website datacard to ensure we have all images downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aff8b92-1b54-4521-8acd-f884092b89ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copied to new folder for analysis and transformations.\n"
     ]
    }
   ],
   "source": [
    "#Creating a copy of the original dataset for our transformation and augmentation works\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source and destination directories\n",
    "src_dir = 'intel_dataset'\n",
    "dest_dir = 'intel_dataset_copy'\n",
    "\n",
    "# Only copy if the destination folder does not exist\n",
    "if not os.path.exists(dest_dir):\n",
    "    shutil.copytree(src_dir, dest_dir)\n",
    "    print(\"Dataset copied to new folder for analysis and transformations.\")\n",
    "else:\n",
    "    print(\"Copy folder already exists. Skipping dataset copy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4419a1ad-e5dd-4903-9263-7ea4594fd9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'intel_dataset_copy' does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Code to delete the copy folder if needed.\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# dest_dir = 'intel_dataset_copy'\n",
    "\n",
    "# if os.path.exists(dest_dir):\n",
    "#    shutil.rmtree(dest_dir)\n",
    "#    print(f\"Folder '{dest_dir}' has been deleted.\")\n",
    "# else:\n",
    "#    print(f\"Folder '{dest_dir}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee9eeb-1656-49ab-8a44-4f310437ffe1",
   "metadata": {},
   "source": [
    "Getting the summary statistics of the images from each of the classes from test and training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e2e2289-6239-41e4-a017-6a3ef7c5413c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== New Training Set Resolution Stats ===\n",
      "\n",
      "Class: Buildings\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "\n",
      "Class: Forest\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 113, Max: 150, Mean: 149.26, Median: 150.0, Std: 5.18\n",
      "\n",
      "Class: Glacier\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 110, Max: 150, Mean: 149.20, Median: 150.0, Std: 5.60\n",
      "\n",
      "Class: Mountain\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "\n",
      "Class: Sea\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "\n",
      "Class: Street\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "\n",
      "=== New Test Set Resolution Stats ===\n",
      "\n",
      "Class: Buildings\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "\n",
      "Class: Forest\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "\n",
      "Class: Glacier\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "\n",
      "Class: Mountain\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "\n",
      "Class: Sea\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "\n",
      "Class: Street\n",
      "  Width  -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "  Height -> Min: 150, Max: 150, Mean: 150.00, Median: 150.0, Std: 0.00\n",
      "\n",
      "=== New Training Set File Size Stats (in KB) ===\n",
      "\n",
      "Class: Buildings\n",
      "  Min: 8.47 KB\n",
      "  Max: 23.09 KB\n",
      "  Mean: 14.87 KB\n",
      "  Median: 14.81 KB\n",
      "  Std: 3.08 KB\n",
      "\n",
      "Class: Forest\n",
      "  Min: 6.31 KB\n",
      "  Max: 23.81 KB\n",
      "  Mean: 19.12 KB\n",
      "  Median: 19.98 KB\n",
      "  Std: 3.31 KB\n",
      "\n",
      "Class: Glacier\n",
      "  Min: 8.90 KB\n",
      "  Max: 18.49 KB\n",
      "  Mean: 13.54 KB\n",
      "  Median: 13.47 KB\n",
      "  Std: 2.59 KB\n",
      "\n",
      "Class: Mountain\n",
      "  Min: 5.21 KB\n",
      "  Max: 21.59 KB\n",
      "  Mean: 12.10 KB\n",
      "  Median: 12.29 KB\n",
      "  Std: 3.62 KB\n",
      "\n",
      "Class: Sea\n",
      "  Min: 5.42 KB\n",
      "  Max: 19.49 KB\n",
      "  Mean: 12.03 KB\n",
      "  Median: 11.83 KB\n",
      "  Std: 3.32 KB\n",
      "\n",
      "Class: Street\n",
      "  Min: 10.12 KB\n",
      "  Max: 19.79 KB\n",
      "  Mean: 15.89 KB\n",
      "  Median: 16.07 KB\n",
      "  Std: 2.50 KB\n",
      "\n",
      "=== New Test Set File Size Stats (in KB) ===\n",
      "\n",
      "Class: Buildings\n",
      "  Min: 11.74 KB\n",
      "  Max: 21.48 KB\n",
      "  Mean: 15.28 KB\n",
      "  Median: 15.00 KB\n",
      "  Std: 2.18 KB\n",
      "\n",
      "Class: Forest\n",
      "  Min: 9.47 KB\n",
      "  Max: 27.27 KB\n",
      "  Mean: 19.51 KB\n",
      "  Median: 20.20 KB\n",
      "  Std: 3.23 KB\n",
      "\n",
      "Class: Glacier\n",
      "  Min: 9.15 KB\n",
      "  Max: 20.40 KB\n",
      "  Mean: 14.43 KB\n",
      "  Median: 14.36 KB\n",
      "  Std: 2.71 KB\n",
      "\n",
      "Class: Mountain\n",
      "  Min: 7.18 KB\n",
      "  Max: 21.05 KB\n",
      "  Mean: 13.09 KB\n",
      "  Median: 13.21 KB\n",
      "  Std: 3.03 KB\n",
      "\n",
      "Class: Sea\n",
      "  Min: 5.91 KB\n",
      "  Max: 22.01 KB\n",
      "  Mean: 13.16 KB\n",
      "  Median: 12.77 KB\n",
      "  Std: 3.27 KB\n",
      "\n",
      "Class: Street\n",
      "  Min: 9.26 KB\n",
      "  Max: 21.34 KB\n",
      "  Mean: 15.81 KB\n",
      "  Median: 15.88 KB\n",
      "  Std: 2.68 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import statistics\n",
    "from PIL import Image\n",
    "\n",
    "# Define functions for computing resolution statistics\n",
    "def image_resolution_stats(directory, sample_limit=50):\n",
    "    stats = {}\n",
    "    classes = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
    "    for cls in classes:\n",
    "        cls_path = os.path.join(directory, cls)\n",
    "        images = sorted([f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        sampled_images = images[:min(sample_limit, len(images))]\n",
    "        widths = []\n",
    "        heights = []\n",
    "        for img_name in sampled_images:\n",
    "            img_path = os.path.join(cls_path, img_name)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    w, h = img.size\n",
    "                    widths.append(w)\n",
    "                    heights.append(h)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "        if widths and heights:\n",
    "            stats[cls] = {\n",
    "                'width_min': min(widths),\n",
    "                'width_max': max(widths),\n",
    "                'width_mean': np.mean(widths),\n",
    "                'width_median': statistics.median(widths),\n",
    "                'width_std': np.std(widths),\n",
    "                'height_min': min(heights),\n",
    "                'height_max': max(heights),\n",
    "                'height_mean': np.mean(heights),\n",
    "                'height_median': statistics.median(heights),\n",
    "                'height_std': np.std(heights)\n",
    "            }\n",
    "    return stats\n",
    "\n",
    "def print_resolution_stats(stats, dataset_name=\"Dataset\"):\n",
    "    print(f\"\\n=== {dataset_name} Resolution Stats ===\")\n",
    "    for cls, s in stats.items():\n",
    "        print(f\"\\nClass: {cls.capitalize()}\")\n",
    "        print(f\"  Width  -> Min: {s['width_min']}, Max: {s['width_max']}, \"\n",
    "              f\"Mean: {s['width_mean']:.2f}, Median: {s['width_median']}, Std: {s['width_std']:.2f}\")\n",
    "        print(f\"  Height -> Min: {s['height_min']}, Max: {s['height_max']}, \"\n",
    "              f\"Mean: {s['height_mean']:.2f}, Median: {s['height_median']}, Std: {s['height_std']:.2f}\")\n",
    "\n",
    "# Define functions for computing file size summary statistics\n",
    "def file_size_stats(directory, sample_limit=50):\n",
    "    stats = {}\n",
    "    classes = sorted([d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))])\n",
    "    for cls in classes:\n",
    "        cls_path = os.path.join(directory, cls)\n",
    "        images = sorted([f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        sampled_images = images[:min(sample_limit, len(images))]\n",
    "        sizes = []\n",
    "        for img_name in sampled_images:\n",
    "            img_path = os.path.join(cls_path, img_name)\n",
    "            try:\n",
    "                size_kb = os.path.getsize(img_path) / 1024  # Convert bytes to kilobytes\n",
    "                sizes.append(size_kb)\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting size for {img_path}: {e}\")\n",
    "        if sizes:\n",
    "            stats[cls] = {\n",
    "                'min': min(sizes),\n",
    "                'max': max(sizes),\n",
    "                'mean': np.mean(sizes),\n",
    "                'median': statistics.median(sizes),\n",
    "                'std': np.std(sizes)\n",
    "            }\n",
    "    return stats\n",
    "\n",
    "def print_file_size_stats(stats, dataset_name=\"Dataset\"):\n",
    "    print(f\"\\n=== {dataset_name} File Size Stats (in KB) ===\")\n",
    "    for cls, s in stats.items():\n",
    "        print(f\"\\nClass: {cls.capitalize()}\")\n",
    "        print(f\"  Min: {s['min']:.2f} KB\")\n",
    "        print(f\"  Max: {s['max']:.2f} KB\")\n",
    "        print(f\"  Mean: {s['mean']:.2f} KB\")\n",
    "        print(f\"  Median: {s['median']:.2f} KB\")\n",
    "        print(f\"  Std: {s['std']:.2f} KB\")\n",
    "\n",
    "# Define new directories for the copied dataset\n",
    "new_train_dir = os.path.join('intel_dataset_copy', 'seg_train', 'seg_train')\n",
    "new_test_dir  = os.path.join('intel_dataset_copy', 'seg_test', 'seg_test')\n",
    "\n",
    "# Compute resolution stats on the new folders\n",
    "new_train_resolution_stats = image_resolution_stats(new_train_dir)\n",
    "new_test_resolution_stats  = image_resolution_stats(new_test_dir)\n",
    "\n",
    "# Print resolution stats\n",
    "print_resolution_stats(new_train_resolution_stats, \"New Training Set\")\n",
    "print_resolution_stats(new_test_resolution_stats, \"New Test Set\")\n",
    "\n",
    "# Compute file size stats on the new folders\n",
    "new_train_file_stats = file_size_stats(new_train_dir)\n",
    "new_test_file_stats  = file_size_stats(new_test_dir)\n",
    "\n",
    "# Print file size stats\n",
    "print_file_size_stats(new_train_file_stats, \"New Training Set\")\n",
    "print_file_size_stats(new_test_file_stats, \"New Test Set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
